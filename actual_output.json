{0: 'Vox2Vox: 3D-GAN for Brain Tumour\nSegmentation\nMarco Domenico Cirillo1,2, David Abramian1,2, and Anders Eklund1,2,3\n1 Department of Biomedical Engineering\n2 Center for Medical Image Science and Visualization\n3 Division of Statistics and Machine learning, Department of Computer and\nInformation Science\nLink¨oping University, Link¨oping, Sweden\nAbstract. We propose a 3D volume-to-volume Generative Adversar-\nial Network (GAN) for segmentation of brain tumours. The proposed\nmodel, called Vox2Vox, generates segmentations from multi-channel 3D\nMR images. The best results are obtained when the generator loss (a 3D\nU-Net) is weighted 5 times higher compared to the discriminator loss (a\n3D GAN). For the BraTS 2018 training set we obtain (after ensembling 5\nmodels) the following dice scores and Hausdorﬀ95 percentile distances:\n90.66%, 82.54%, 78.71%, and 4.04 mm, 6.07 mm, 5.00 mm, for whole\ntumour, core tumour and enhancing tumour respectively. The proposed\nmodel is shown to compare favorably to the winners of the BraTS 2018\nchallenge, but a direct comparison is not possible.\nKeywords: MRI · Vox2Vox · Generative Adversarial Networks · deep\nlearning · artiﬁcial intelligence · 3D image segmentation.\n1\nIntroduction\nGliomas are the most frequent intrinsic tumours of the central nervous system\nand encompass two principle subgroups: diﬀuse gliomas (high grade gliomas,\nHGG), and gliomas showing a more circumscribed growth pattern (low grade\ngliomas, LGG) [31]. Although both these brain tumours types can be detected,\nthey have a diﬀuse, inﬁltrative way of growing in the brain, and they exhibit\nperitumoural oedema, such as an increase in water content in the area surround-\ning the tumour. This makes it arduous to deﬁne the tumour border by visual\nassessment, both in analysis and also during surgery [5].\nFor this reason, researchers started recently to resort to powerful techniques,\nable to segment complex objects and, in this way, guide the surgeons during\nthe operation with a suitable accuracy. Indeed, machine learning [25] and deep\nlearning [12,14,17,20,23,29] can provide state-of-the-art segmentation results.\n1.1\nRelated Works\nNowadays generative adversarial networks (GANs) [9] are gaining popularity in\ncomputer vision, since they can learn to synthesise virtually any type of image.\narXiv:2003.13653v1 [cs.CV] 19 Mar 2020', 1: '2\nM.D.Cirillo et al.\nSpeciﬁcally, GANs can be used for style transfer [8], image synthesis from noise\n[15], image to image translation [13], and also image segmentation [27]. GANs\nhave become especially popular in medical imaging [32] since medical imaging\ndatasets are much smaller compared to general computer vision datasets such as\nImageNet. Additionally, it is common to collect several image modalities for each\nsubject before proceeding with the analysis, and, when this cannot be possible,\nCycleGAN introduced in [33] can be used to synthesize those missing modalities.\nGANs have also been used for medical image segmentation. Indeed, Z. Han\net al. in [10] proposed a GAN to segment multiple spinal structures in MRIs; Y.\nLi et al. in [19] developed a novel transfer-learning framework using a GAN for\nrobust segmentation of diﬀerent human epithelial type 2 (HEp-2) cells; X. Dong\net al. in [6] implemented a U-Net style GAN for accurate and timely organs-\nat-risk (OARs) segmentation; S. Nema et al. in [24] designed a 2D GAN, called\nRescueNet, to segment brain tumours from MR images; etc. Yi et al. [32] provide\na complete and recent review of GANs applied in medicine.\nHence, inspired by these works and especially by the Pix2Pix GAN [13],\nwhich can generate an image of type A from an image of type B, the aim of\nthis project is to do 3D image segmentation using 3D Pix2Pix GAN, named\nVox2Vox, to segment brain gliomas. While a normal convolutional neural net-\nwork, such as U-Net [26], performs the segmentation pixel by pixel, or voxel by\nvoxel, through maximizing a segmentation metric or metrics (i.e. dice score, in-\ntersection over union, etc), a GAN will also punish segmentation results that do\nnot look realistic. Our hypothesis is that this can result in better segmentations.\n2\nMethod\n2.1\nData\nThe MR images used for this project are the Multimodal Brain tumour Seg-\nmentation Challenge (BraTS) 2018 training ones [1,2,3,4,21]. The BraTS 2018\ntraining dataset contains MR volumes of shape 240 × 240 × 155 from 285 pa-\ntients, and for each patient four types of MR images were collected: native (T1),\npost-contrast T1-weighted (T1Gd), T2-weighted (T2), and T2 Fluid Attenuated\nInversion Recovery (FLAIR). The data were acquired from 19 diﬀerent institu-\ntions with diﬀerent clinical protocols. The tumours have been segmented man-\nually, by one to four raters, following the same annotation protocol, and their\nannotations were approved by experienced neuro-radiologists. Figure 1 shows an\nexample of one of the test volumes.\n2.2\nImage Pre-processing\nFor each MR image intensity normalization is done per channel, whereas the\nbackground voxels are ﬁxed to 0. On the other hand, the grey-scale ground-\ntruths are transformed into categorical, so each target has four channels, as\nthe number of the classes to segment: background, peritumoural edema (ED),', 2: 'Vox2Vox: 3D-GAN for Brain Tumour Segmentation\n3\nFig. 1. From left to right there is a T1-weighted MR image in the sagittal, coronal\nand transverse plane overlapped with its ground-truth segmentations. Peritumoural\nedema (ED), necrotic and non-enhancing tumour core (NCR/NET), and GD-enhancing\ntumour (ET) are highlighted in yellow, cyan and red respectively.\nnecrotic and non-enhancing tumour core (NCR/NET), and GD-enhancing tu-\nmour (ET) labeled with 0, 1, 2, 3 respectively.\nSince the BraTS volumes are memory demanding, patch augmentation is\napplied to extract one sub-volume of 128 × 128 × 128 from each original volume.\nIn this way, only 23.5% of the whole training set is used in every training epoch.\nThe training, validation and testing sets are split into 182 (64%), 46 (16%)\nand 57 (20%) volumes respectively.\n2.3\nModel Architecture\nThe Vox2Vox model, as the Pix2Pix one [13], consists of a generator and a\ndiscriminator. The generator, illustrated by Figure 2, is built as U-Net [26]:\nI: a 3D image with 4 channels: T1, T2, T1Gd, and T2 FLAIR;\nE: four 3D convolutions using kernel size 4 × 4 × 4, stride 2 and same padding,\nfollowed by instance normalization [30] and Leaky ReLU activation function.\nThe number of ﬁlters used at the ﬁrst 3D convolution is 64 and at each down-\nsampling the number is doubled;\nB: four 3D convolutions using kernel size 4 × 4 × 4, stride 1 and same padding,\nfollowed by instance normalization and Leaky ReLU activation function.\nEvery convolution-normalization-activation output is concatenated with the\nprevious one;\nD: three 3D transpose convolutions using kernel size 4 × 4 × 4 and stride 2, fol-\nlowed by instance normalization and ReLU activation. Each 3D convolution\ninput is concatenated with the respective encoder output layer;\nO: segmentation prediction of size 128 × 128 × 128 × 4 given by a 3D transpose\nconvolution using 4 ﬁlters (as the number of the classes to segment), kernel\nsize 4 ×4×4 and stride 2, followed by softmax activation function generates\n4 channel segmentation prediction of the input image.\nOn the other hand, the discriminator consists of:\nI: the 3D image with 4 channels and its segmentation ground-truth or the\ngenerator’s segmentation prediction;', 3: '4\nM.D.Cirillo et al.\nE: the same of the generator;\nO: volume of size 8 × 8 × 8 × 1 given by a 3D convolution using 1 ﬁlter, kernel\nsize 4×4×4, stride 1 and same padding generates the discriminator output,\nused to determine the quality of the segmentation prediction created by the\ngenerator.\n128x128x128x4\n3D-Conv, kernel size 4x4x4, stride 2, \nInstance Norm, Leaky ReLU\n64x64x64x64\n32x32x32x128\n16x16x16x256\n8x8x8x512\n8x8x8x512\n8x8x8x512\n8x8x8x512\n8x8x8x512\n3D-Conv, kernel size 4x4x4, stride 1, \nInstance Norm, Leaky ReLU, Dropout 0.2\n16x16x16x512\n3D-TransposeConv, kernel size 4x4x4,\nstride 2, Instance Norm, ReLU\n128x128x128x4\n32x32x32x256\n64x64x64x128\nConcatenation\n3D-TransposeConv, kernel size 4x4x4, \nstride 2, softmax\nFig. 2. The generator model with 3D U-Net architecture style.\nAll the kernel weights for each 3D convolution are initialized using the He et\nal. method [11] and all Leaky ReLU layers have slope coeﬃcient 0.3. On the other\nhand, I, E, B, D, and O here stand for input(s), encoder, bottleneck, decoder,\nand output respectively.\n2.4\nLosses\nSince Vox2Vox contains two models, the generator and the discriminator, two\nloss functions are used. The discriminator loss, LD, is the sum between the\nL2 error of the discriminator output, D(·, ·), between the original image x and\nthe respective ground-truth y with a tensor of ones, and the L2 error of the\ndiscriminator output between the original image and the respective segmentation\nprediction ˆy given by the generator with a tensor of zeros, i.e.:\nLD = L2 [D(x, y), 1] + L2 [D(x, ˆy), 0] ,\n(1)\nwhereas, the generator loss, LG, is the sum between the L2 error of the\ndiscriminator output between the original image and the respective segmentation\nprediction given by the generator with a tensor of ones, and the generalized dice\nloss [22,28], GDL(·, ·), between the ground-truth and the generator’s output\nmultiplied by the scalar α ≥0, i.e.:\nLG = L2 [D(x, ˆy), 1] + α GDL [y, ˆy] .\n(2)', 4: 'Vox2Vox: 3D-GAN for Brain Tumour Segmentation\n5\nBy looking at Equation 2, it is easy to conclude that if α = 0: Vox2Vox is\na pure GAN and it minimizes only the unsupervised loss given by the discrim-\ninator; whereas if α →∞: Vox2Vox ignores the discriminator output, and so it\nbehaves more as a 3D U-Net.\n2.5\nOptimization and Regularization\nBoth the generator and the discriminator are trained using the Adam optimizer\n[16] with the parameters: λ = 2 · 10−4, β1 = 0.5, and β2 = 0.999. Dropout regu-\nlarization with a dropout probability of 0.2 is used after each 3D convolutional\noperation in the generator’s bottleneck (see Figure 2). Moreover, as X. Yi re-\nported in [32], the discriminator loss helps the generator to guarantee the spatial\nconsistency in the ﬁnal results, behaving as a shape regularizer. In other words,\nthe discriminator takes care that the generated brain segmentation looks realistic\n(i.e. like manual segmentations). In the end, we expect that Vox2Vox performs\nbetter with a trade-oﬀα which does not disregard completely the discriminator\nloss and, at the same time, does not disregard the generator either.\n2.6\nModel Ensembling\nOnce the network is implemented, it is possible to do M-fold cross-validation\nand then ensemble all the models in order to have a more robust segmentation\nprediction [14]. The ensemble is simply done by averaging the class probabili-\nties voxel-per-voxel, given by the softmax activation function, of the predictions\nproduced by each cross-validation model, i.e.:\nˆyE = P(y|x) = 1\nM\nM\nX\ni=1\nP(y|x, mi),\n(3)\nwhere ˆyE is the ensemble prediction of the target y given the MR images\nx, whereas P(y|x, mi), with i = 1, ..., M and M is the number of models to\nensemble, is the prediction of the target y given the MR images x using the i-th\nmodel m.\n3\nResults\nThe Vox2Vox model is implemented using Python 3.7, Tensorﬂow 2.1 and its\nKeras library. The code is available on Github4.\nThe model is trained and validated on sub-volumes of size 128 × 128 × 128\nfrom 182 and 46 subjects respectively, using batch size 4, over 200 epochs on\na computer equipped with 128 GB RAM and an Nvidia GeForce RTX 2080 Ti\ngraphics card with 11 GB of memory. Once the training is completed, the 57\ntesting volumes are cropped in order to have shape 160 × 192 × 128. In this\n4 https://github.com/mdciri/Vox2Vox', 5: '6\nM.D.Cirillo et al.\nway, the testing set can be given as input to the fully convolutional Vox2Vox,\nbecause each axis is now divisible by 24 = 16, where 4 is the generator’s and\ndiscriminator’s depth.\nAfter many attempts, we decided that a good trade-oﬀto weight the dis-\ncriminator and the generator is when α = 5. Table 1 reports the dice and the\nHausdorﬀdistance 95 percentile scores between the ground-truth and the gen-\nerated predictions over the testing set. Note that the tumour’s classes are reset\nas: whole tumour (WT = ET ∪NCR/NET ∪ED), tumour core (TC = ET ∪\nNCR/NET) and enhancing tumour (ET).\nTable 1. Dice score and Hausdorﬀdistance 95 percentile for the diﬀerent brain tumour\nareas for the testing set. The metrics here are obtained when training the model with\nsub-volumes of 128 × 128 × 128 voxels, α = 5, and with image augmentation: random\npatch extraction (PE), ﬂipping (F) and rotation (R).\nDice score [%]\nHausdorﬀdistance 95 [mm]\nAug. tech.\nWT\nTC\nET\nWT\nTC\nET\nPE\n88.62\n78.30\n74.81\n6.82\n7.79\n7.66\nPE + F\n88.76\n78.78\n73.82\n8.19\n9.34\n8.75\nPE + R\n89.66\n77.64\n74.35\n6.23\n11.55\n9.03\nPE + F + R\n89.73\n81.02\n77.46\n4.60\n6.97\n5.00\nTable 1 also reports the scores when applying random ﬂipping, random rota-\ntions between 0◦and 30◦, or both in the three spatial axes as image augmentation.\nThe best choice is to apply both ﬂipping and rotation as image augmentation,\nbecause they increase substantially all the metrics for each class of interest. It\ntakes between 8 and 9 minutes to complete one epoch when extracting batches\nfrom a Keras generator (reading volumes from the hard drive), or between 4 and\n5 minutes if all training and validation sub-volumes are stored in CPU memory.\nIn the end, training and validation set are merged and then divided into\n5 folds in order to perform 5-fold cross-validation on the Vox2Vox trained on\nsub-volumes of 128 × 128 × 128 voxels, using α = 5 and applying ﬂipping and\nrotation as augmentation. Table 2 reports the scores obtained from such cross-\nvalidation and also those from the ensemble of these 5 models, as explained in\nEquation 3. Figure 3 shows an example of segmentation prediction realized by\nsuch ensembling.\nIt is interesting to notice in Table 2 that even if the ﬁfth model behaves worse\nthan the others, this does not severely aﬀect the ensemble’s performance which\nis better than every single model’s one. Moreover, Table 2 also compares the', 6: 'Vox2Vox: 3D-GAN for Brain Tumour Segmentation\n7\nTable 2. Dice score and Hausdorﬀdistance 95 percentile for the diﬀerent brain tumour\nareas on the testing set. The metrics reported here are the average (avg) and standard\ndeviation (std) for each class after 5-fold cross-validation (CV). These metrics are\nextracted after ensembling these 5 models trained on sub-volumes of 128 × 128 × 128\nvoxels, using α = 5 and applying both ﬂip and rotation as image augmentation.\nDice score [%]\nHausdorﬀdistance 95 [mm]\nWT\nTC\nET\nWT\nTC\nET\nmodel 1\n89.73\n81.02\n77.46\n4.60\n6.97\n5.00\nmodel 2\n90.19\n81.64\n77.89\n4.38\n5.95\n5.00\nmodel 3\n89.42\n78.37\n75.01\n5.26\n8.49\n7.07\nmodel 4\n90.34\n80.07\n77.62\n7.37\n7.73\n6.07\nmodel 5\n86.57\n70.69\n72.78\n5.51\n10.52\n6.48\nCV avg\n89.25\n78.36\n76.15\n5.42\n7.93\n5.92\nCV std\n1.37\n3.99\n1.97\n1.05\n1.54\n0.81\nensemble\n90.66\n82.54\n78.71\n4.04\n6.07\n5.00\nA. Myronenko [23]\n88.39\n81.54\n76.64\n5.90\n4.81\n3.77\nF. Isensee et al. [12]\n87.81\n80.62\n77.88\n6.03\n5.08\n2.90\nobtained segmentation metrics with those reported in [12] and [23], winners of the\n2nd and 1st BraTS challenge 2018 place respectively. Those results were obtained\nensembling 10 and 2 models (ensembled by 5 other models each) respectively and\ntraining, validating and testing on 285 (our whole dataset), 66 and 191 3D MR\nimages, so an exact comparison cannot be done. Anyway, ensembling 5 Vox2Vox\nmodels into 1 model returns a stronger network able to segment brain tumours\nwith better dice scores, almost 1-2% better for each class, but worse Hausdorﬀ\ndistances for the core and enhancing tumour. We think that this might be due\nto shape regularization introduced by the discriminator.\n4\nConclusion\nFigure 3 and Table 2 establish that the ensemble of multiple Vox2Vox models re-\nturn high quality segmentation outputs: achieving 90.66%, 82.54%, 78.71% dice,\nand 4.04mm, 6.07mm, 5.00mm Hausdorﬀdistance 95 percentile scores for whole\ntumour, core tumour and enhancing tumour respectively. The values reported', 7: '8\nM.D.Cirillo et al.\nFig. 3. Top: an example of a T1-weighted MR image in the sagittal, coronal and trans-\nverse plane overlapped with its ground-truth segmentation. Bottom: the predicted seg-\nmentation made by the ensemble model. Peritumoural edema (ED), necrotic and non-\nenhancing tumour core (NCR/NET), and GD-enhancing tumour (ET) are highlighted\nin yellow, cyan and red respectively.\nare better than those of the ﬁrst two winners of the BraTS 2018 challenge, but\nas previously explained a direct comparison is not possible.\nThese values compare favorably to the ﬁrst two winners of the BraTS 2018\nchallenge, but at the same time those networks were trained and tested on more\ndata than here and more models were used for the ensemble.\nIn the end, the Vox2Vox model can be used not only for image segmentation\nbut also for further image augmentation. Indeed, Vox2Vox could be combined\nwith a 3D noise to image GAN [7,18] which can synthesize realistic ground\ntruth segmentations, that are then translated to realistic MR volumes. This\ncombination might result in a fast batch generator of MR images, with ground-\ntruth segmentations for training another network. It would be interesting to\nparticipate in the next BraTS challenge with the Vox2Vox model, in order to\nevaluate it with an ampler brain tumour image dataset than the one used here.\nAcknowledgement\nThis study was supported by VINNOVA Analytic Imaging Diagnostics Arena\n(AIDA) and the ITEA3 / VINNOVA funded project Intelligence based iMprove-\nment of Personalized treatment And Clinical workﬂow supporT (IMPACT).\nFunding was also provided by the Center for Industrial Information Technol-\nogy (CENIIT) at Linkping University.', 8: 'Vox2Vox: 3D-GAN for Brain Tumour Segmentation\n9\nReferences\n1. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,\nJ., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic features for the\npre-operative scans of the TCGA-GBM collection. The Cancer Imaging Archive\n(2017)\n2. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,\nJ., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic features for the\npre-operative scans of the TCGA-LGG collection. The Cancer Imaging Archive\n286 (2017)\n3. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Freymann,\nJ.B., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma MRI\ncollections with expert segmentation labels and radiomic features. Scientiﬁc data\n4, 170117 (2017)\n4. Bakas, S., Reyes, M., Jakab, A., Bauer, S., Rempﬂer, M., Crimi, A., Shinohara,\nR.T., Berger, C., Ha, S.M., Rozycki, M., et al.: Identifying the best machine learn-\ning algorithms for brain tumor segmentation, progression assessment, and over-\nall survival prediction in the BRATS challenge. arXiv preprint arXiv:1811.02629\n(2018)\n5. Blystad, I.: Clinical Applications of Synthetic MRI of the Brain, vol. 1600.\nLink¨oping University Electronic Press (2017)\n6. Dong, X., Lei, Y., Wang, T., Thomas, M., Tang, L., Curran, W.J., Liu, T., Yang,\nX.: Automatic multiorgan segmentation in thorax CT images using U-net-GAN.\nMedical physics 46(5), 2157–2168 (2019)\n7. Eklund, A.: Feeding the zombies: Synthesizing brain volumes using a 3D progressive\ngrowing GAN. arXiv:1912.05357 (2019)\n8. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional\nneural networks. In: Proceedings of the IEEE conference on computer vision and\npattern recognition. pp. 2414–2423 (2016)\n9. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural\ninformation processing systems. pp. 2672–2680 (2014)\n10. Han, Z., Wei, B., Mercado, A., Leung, S., Li, S.: Spine-GAN: Semantic segmenta-\ntion of multiple spinal structures. Medical image analysis 50, 23–35 (2018)\n11. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-\nlevel performance on imagenet classiﬁcation. In: Proceedings of the IEEE interna-\ntional conference on computer vision. pp. 1026–1034 (2015)\n12. Isensee, F., Kickingereder, P., Wick, W., Bendszus, M., Maier-Hein, K.H.: No new-\nnet. In: International MICCAI Brainlesion Workshop. pp. 234–244. Springer (2018)\n13. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image–to–image translation with condi-\ntional adversarial networks. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 1125–1134 (2017)\n14. Kamnitsas, K., Bai, W., Ferrante, E., McDonagh, S., Sinclair, M., Pawlowski, N.,\nRajchl, M., Lee, M., Kainz, B., Rueckert, D., et al.: Ensembles of multiple models\nand architectures for robust brain tumour segmentation. In: International MICCAI\nBrainlesion Workshop. pp. 450–462. Springer (2017)\n15. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of GANs for\nimproved quality, stability, and variation. ICLR (2018)\n16. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)', 9: '10\nM.D.Cirillo et al.\n17. Kong, X., Sun, G., Wu, Q., Liu, J., Lin, F.: Hybrid pyramid U-Net model for\nbrain tumor segmentation. In: International Conference on Intelligent Information\nProcessing. pp. 346–355. Springer (2018)\n18. Kwon, G., Han, C., Kim, D.s.: Generation of 3D brain MRI using auto-encoding\ngenerative adversarial networks. In: International Conference on Medical Image\nComputing and Computer-Assisted Intervention. pp. 118–126 (2019)\n19. Li, Y., Shen, L.: cC-GAN: A robust transfer-learning framework for HEp-2 speci-\nmen image segmentation. IEEE Access 6, 14048–14058 (2018)\n20. McKinley, R., Meier, R., Wiest, R.: Ensembles of densely-connected CNNs with\nlabel-uncertainty for brain tumor segmentation. In: International MICCAI Brain-\nlesion Workshop. pp. 456–465. Springer (2018)\n21. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor\nimage segmentation benchmark (BRATS). IEEE transactions on medical imaging\n34(10), 1993–2024 (2014)\n22. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 2016 Fourth International Confer-\nence on 3D Vision (3DV). pp. 565–571. IEEE (2016)\n23. Myronenko, A.: 3D MRI brain tumor segmentation using autoencoder regular-\nization. In: International MICCAI Brainlesion Workshop. pp. 311–320. Springer\n(2018)\n24. Nema, S., Dudhane, A., Murala, S., Naidu, S.: RescueNet: An unpaired GAN for\nbrain tumor segmentation. Biomedical Signal Processing and Control 55, 101641\n(2020)\n25. Polly, F., Shil, S., Hossain, M., Ayman, A., Jang, Y.: Detection and classiﬁcation\nof HGG and LGG brain tumor using machine learning. In: 2018 International\nConference on Information Networking (ICOIN). pp. 813–817. IEEE (2018)\n26. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n27. Sato, M., Hotta, K., Imanishi, A., Matsuda, M., Terai, K.: Segmentation of Cell\nMembrane and Nucleus by Improving Pix2pix. In: BIOSIGNALS. pp. 216–220\n(2018)\n28. Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Cardoso, M.J.: Generalised\ndice overlap as a deep learning loss function for highly unbalanced segmentations.\nIn: Deep learning in medical image analysis and multimodal learning for clinical\ndecision support, pp. 240–248. Springer (2017)\n29. Topol, E.J.: High-performance medicine: the convergence of human and artiﬁcial\nintelligence. Nature medicine 25(1), 44–56 (2019)\n30. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Instance normalization: The missing in-\ngredient for fast stylization. arXiv preprint arXiv:1607.08022 (2016)\n31. Wesseling, P., Capper, D.: Who 2016 classiﬁcation of gliomas. Neuropathology and\napplied neurobiology 44(2), 139–150 (2018)\n32. Yi, X., Walia, E., Babyn, P.: Generative adversarial network in medical imaging:\nA review. Medical image analysis p. 101552 (2019)\n33. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: Proceedings of the IEEE interna-\ntional conference on computer vision. pp. 2223–2232 (2017)'}